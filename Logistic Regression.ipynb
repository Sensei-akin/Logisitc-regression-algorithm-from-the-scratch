{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing important libraries\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n",
    "\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "train = pd.read_csv(train_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([('std_scaler', StandardScaler())])\n",
    "cat_pipeline = Pipeline([('one_hot_encoding', OneHotEncoder(handle_unknown = \"ignore\", sparse = False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_attributes = train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "num_attributes.remove(\"survived\")\n",
    "cat_attributes = train.select_dtypes(exclude=[np.number]).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['survived']\n",
    "features = train.drop([\"survived\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pipeline = ColumnTransformer([\n",
    "            (\"num\", num_pipeline, num_attributes),\n",
    "            (\"cat\", cat_pipeline, cat_attributes)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = full_pipeline.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets implement the following\n",
    "\n",
    "#Activation (sigmoid function)\n",
    "def sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "\n",
    "#using multiple linear models or perceptrons\n",
    "#differential of sigmoid function is \n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x)* (1-sigmoid(x))\n",
    "\n",
    "\n",
    "#Ouptut(prediction function)\n",
    "def prediction(features, weight,bias):\n",
    "    return sigmoid(np.dot(features,weight)+ bias)\n",
    "\n",
    "\n",
    "#error (log-loss function) for a single perceptron\n",
    "def error_formula(y,output):\n",
    "    return -y*np.log(output) - (1-y)*np.log(1-output)\n",
    "\n",
    "#error (log-loss function) for multiple perceptron\n",
    "def error_fucn(x,y,output):\n",
    "    differential = sigmoid_prime(x)\n",
    "    error = y - output\n",
    "    return error*differential\n",
    "\n",
    "\n",
    "#gradient descent step\n",
    "def update_weights(x,y, weight,bias, learnrate):\n",
    "    pred = prediction(x,weight,bias)\n",
    "    error = y - pred\n",
    "    weight += learnrate * error * x\n",
    "    bias += learnrate * error\n",
    "    return weight,bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seed random numbers to make calculation deterministic\n",
    "np.random.seed(45)\n",
    "\n",
    "def training(features,targets,epochs,learnrate):\n",
    "    errors = []\n",
    "    last_loss = None\n",
    "    #assign a random weight\n",
    "    n_records, n_features = features.shape\n",
    "    weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "    bias = 0\n",
    "    for iter in range(epochs):\n",
    "        for x,y in zip(features,targets):\n",
    "            output = prediction(x,weights,bias)\n",
    "            error = y - output\n",
    "            weights, bias = update_weights(x,y,weights,bias,learnrate)\n",
    "        \n",
    "        #print out log loss for training dataset\n",
    "        out = prediction(features,weights,bias)\n",
    "        loss = np.mean(error_formula(target,out))\n",
    "        errors.append(loss)\n",
    "        if iter % (epochs/10) == 0:\n",
    "            print('\\n==============Epoch', iter, \"========\")\n",
    "        if last_loss and last_loss < loss:\n",
    "            print('Training loss:', loss, \" WARNING ---  Loss is increasing\")\n",
    "        else:\n",
    "            print('Training loss', loss)\n",
    "        last_loss  = loss\n",
    "        predictions = out > 0.5\n",
    "        accuracy = np.mean(predictions==targets)\n",
    "        print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============Epoch 0 ========\n",
      "Training loss 0.4851238446298384\n",
      "Accuracy:  0.7719298245614035\n",
      "\n",
      "==============Epoch 1 ========\n",
      "Training loss 0.44884250790371155\n",
      "Accuracy:  0.8118022328548644\n",
      "\n",
      "==============Epoch 2 ========\n",
      "Training loss 0.43517567657268796\n",
      "Accuracy:  0.8118022328548644\n",
      "\n",
      "==============Epoch 3 ========\n",
      "Training loss 0.42892098265419376\n",
      "Accuracy:  0.8118022328548644\n",
      "\n",
      "==============Epoch 4 ========\n",
      "Training loss 0.42565698585045747\n",
      "Accuracy:  0.8165869218500797\n",
      "\n",
      "==============Epoch 5 ========\n",
      "Training loss 0.42376789901305173\n",
      "Accuracy:  0.8149920255183413\n",
      "\n",
      "==============Epoch 6 ========\n",
      "Training loss 0.4225761643002155\n",
      "Accuracy:  0.8165869218500797\n",
      "\n",
      "==============Epoch 7 ========\n",
      "Training loss 0.4217674714406743\n",
      "Accuracy:  0.8149920255183413\n",
      "\n",
      "==============Epoch 8 ========\n",
      "Training loss 0.4211837570164918\n",
      "Accuracy:  0.8149920255183413\n",
      "\n",
      "==============Epoch 9 ========\n",
      "Training loss 0.42073998986640326\n",
      "Accuracy:  0.8165869218500797\n"
     ]
    }
   ],
   "source": [
    "training(features,target,10,0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
